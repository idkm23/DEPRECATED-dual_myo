'''
Created on Jan 7, 2016

@author: ymeng
'''

import collections
import numpy as np

SMOOTHING = 0.1

class State(object):
    def __init__(self, label=None, imu=None):
        self.label = label
        self.imu = imu
        if imu:
            self.orientation = imu[6:8]

class BuildMDP(object):
    '''
    Build transition matrices and reward matrices
    '''
    def __init__(self, actionsFile, statesFile):
        '''
        Constructor
        '''
        self.actionsFile = actionsFile
        self.statesFile = statesFile
        
    def getTransitions(self):
        
        C3 = collections.defaultdict(int)
        C2 = collections.defaultdict(int)
        self.P = {}
        
        with open(self.actionsFile) as f:
            actions = ['a'+x.strip() for x in f]
        #--------------------------------------------------------- print actions
        with open(self.statesFile) as f:
            states = ['s'+x.strip() for x in f]
        #---------------------------------------------------------- print states
        
        assert len(actions) == len(states)
        for i in range(len(states)):
            #----------------------------------------------------------- print i
            C3[(actions[i], states[i], states[i+1])] += 1
            C2[(actions[i], states[i])] += 1
            if i == len(states)-2: break
        
        for key in C3:
            self.P[key] = (SMOOTHING+C3[key]) / (SMOOTHING*len(C2)+C2[(key[0], key[1])])
        
        self.actions = sorted(list(set(actions)))
        self.states = sorted(list(set(states)))
    
    def getProb(self, a, s, s_next):
        
        if (a, s, s_next) in self.P:
            return self.P[a, s, s_next]
        else:
            # return SMOOTHING / (SMOOTHING*(len(self.actions) + len(self.states)) + self.P.get((a, s), 0))
            return 0
        
    def buildP(self):
        """
        Build the transition probability matrices
        """
        n_actions = len(self.actions)
        n_states = len(self.states)
        self.T = np.zeros((n_actions, n_states, n_states))
        
        for i in range(n_actions):
            for j in range(n_states):
                for k in range(n_states):
                    self.T[i, j, k] = self.getProb(self.actions[i], self.states[j], self.states[k])
    
    def findPath(self, start_state, end_state):
        current_state = start_state
        path = []
        while True:
            path.append(current_state)
            if current_state == end_state or len(path)>10:
                break
            current_idx = self.states.index(current_state)
            T = self.T[:,current_idx,:]
            # do not consider transition to itself here
            T[:, current_idx] = 0
            next_idx = np.unravel_index(T.argmax(), T.shape)[1]
            current_state = self.states[next_idx]
        return path
           
    def getReward(self, a, s, s_next):
        prob = self.T[a, s, s_next]
        
        
            
        
    
                
if __name__ == '__main__':
    actionsFile = './data/emg_labels'
    statesFile = './data/imu_labels'
    builder = BuildMDP(actionsFile, statesFile)
    builder.getTransitions()
    print builder.actions, builder.states
    builder.buildP()
    print builder.T
                      